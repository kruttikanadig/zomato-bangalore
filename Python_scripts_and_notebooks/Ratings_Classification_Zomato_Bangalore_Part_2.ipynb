{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is Part Two of my analysis of the Zomato Bangalore dataset. In Part One we attempted to predict restaurant ratings with six selected features using regression. In this kernel we will keep the same features, but transform the ratings into a categorical target with four levels and build classification models to predict them. \n",
    "\n",
    "Exploratory Data Analysis was done in Part One. This kernel consists of:\n",
    "\n",
    "- Data cleaning (identifying and dropping duplicates, reformatting features)\n",
    "- Preprocessing and prediction with Decision Tree, Random Forest and XGBoost \n",
    "- Model evaluation (Accuracy, Cohen Kappa, F1 score, Precision, Recall)\n",
    "- Feature Importance visualization\n",
    "- Results summary\n",
    "\n",
    "We will go through the data cleaning and preprocessing quickly - see Part One for explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, cohen_kappa_score, classification_report\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "zomato = pd.read_csv(\"../input/zomato.csv\", na_values = [\"-\", \"\"])\n",
    "# Making a copy of the data to work on\n",
    "data = zomato.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# Renaming and removing commas in the cost column \n",
    "data = data.rename({\"approx_cost(for two people)\": \"cost\"}, axis=1)\n",
    "data[\"cost\"] = data[\"cost\"].replace(\",\", \"\", regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting numeric columns to their appropriate dtypes\n",
    "data[[\"votes\", \"cost\"]] = data[[\"votes\", \"cost\"]].apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group and aggregate duplicate restaurants that are listed under multiple types in listed_in(type)\n",
    "grouped = data.groupby([\"name\", \"address\"]).agg({\"listed_in(type)\" : list})\n",
    "newdata = pd.merge(grouped, data, on = ([\"name\", \"address\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows which have duplicate information in \"name\", \"address\" and \"listed_in(type)_x\"\n",
    "newdata[\"listed_in(type)_x\"] = newdata[\"listed_in(type)_x\"].astype(str) # converting unhashable list to a hashable type\n",
    "newdata.drop_duplicates(subset = [\"name\", \"address\", \"listed_in(type)_x\"], inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the restaurant names to rownames \n",
    "newdata.index = newdata[\"name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping unnecessary columns\n",
    "newdata.drop([\"name\", \"url\", \"phone\", \"listed_in(city)\", \"listed_in(type)_x\", \"address\", \"dish_liked\",  \"listed_in(type)_y\", \"menu_item\", \"cuisines\", \"reviews_list\"], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming the target (restaurant ratings)\n",
    "\n",
    "# Extracting the first three characters of each string in \"rate\"\n",
    "newdata[\"rating\"] = newdata[\"rate\"].str[:3] \n",
    "# Removing rows with \"NEW\" in ratings as it is not a predictable level\n",
    "newdata = newdata[newdata.rating != \"NEW\"] \n",
    "# Dropping rows that have missing values in ratings \n",
    "newdata = newdata.dropna(subset = [\"rating\"])\n",
    "# Converting ratings to a numeric column so we can discretize it\n",
    "newdata[\"rating\"] = pd.to_numeric(newdata[\"rating\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our EDA in Part One showed that 3.7 is the most common rating and the frequency of ratings below 2.5 and above 4 is very low. To prevent a class imbalance problem, we will create custom-sized bins that take frequency counts into account while still making sense.\n",
    "\n",
    "Our four rating bins (classes) will be 0 to 3 < 3 to 3.5 < 3.5 to 4 < 4 to 5. To make label encoding easier later, we'll label these classes 0, 1, 2, 3. **We can think of these as Very Low, Low, Medium and High.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discretizing the ratings into a categorical feature with 4 classes\n",
    "newdata[\"rating\"] = pd.cut(newdata[\"rating\"], bins = [0, 3.0, 3.5, 4.0, 5.0], labels = [\"0\", \"1\", \"2\", \"3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the number of restaurants in each rating class\n",
    "np.unique(newdata[\"rating\"], return_counts = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 884 restaurants in rating class 0 (Very Low), 2929 in class 1 (Low), 4037 in class 2 (Medium) and 1466 in class 4 (High)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the original rating column\n",
    "newdata.drop(\"rate\", axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata.describe(include = \"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating the predictors and target\n",
    "predictors = newdata.drop(\"rating\", axis = 1)\n",
    "target = newdata[\"rating\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(predictors, target, random_state = 0, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the predictors\n",
    "num_cols = [\"votes\", \"cost\"]\n",
    "cat_cols = [\"location\", \"rest_type\", \"online_order\", \"book_table\"]\n",
    "\n",
    "num_imputer = SimpleImputer(strategy = \"median\")\n",
    "# Imputing numeric columns with the median (not mean because of the high variance)\n",
    "num_imputed = num_imputer.fit_transform(X_train[num_cols])\n",
    "scaler = StandardScaler()\n",
    "# Scaling the numeric columns to have a mean of 0 and standard deviation of 1\n",
    "num_preprocessed = pd.DataFrame(scaler.fit_transform(num_imputed), columns = num_cols)\n",
    "\n",
    "cat_imputer = SimpleImputer( strategy = \"most_frequent\")\n",
    "# Imputing categorical columns with the mode\n",
    "cat_imputed = pd.DataFrame(cat_imputer.fit_transform(X_train[cat_cols]), columns = cat_cols)\n",
    "# Dummifying the categorical columns\n",
    "cat_preprocessed = pd.DataFrame(pd.get_dummies(cat_imputed, prefix = cat_cols, drop_first = True))\n",
    "\n",
    "train_predictors = pd.concat([num_preprocessed, cat_preprocessed], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_num_imputed = num_imputer.transform(X_test[num_cols])\n",
    "test_num_preprocessed = pd.DataFrame(scaler.transform(test_num_imputed), columns = num_cols)\n",
    "\n",
    "test_cat_imputed = pd.DataFrame(cat_imputer.transform(X_test[cat_cols]), columns = cat_cols)\n",
    "test_cat_preprocessed = pd.DataFrame(pd.get_dummies(test_cat_imputed, prefix = cat_cols, drop_first = True))\n",
    "                                    \n",
    "test_predictors = pd.concat([test_num_preprocessed, test_cat_preprocessed], axis=1)\n",
    "\n",
    "# Accounting for missing columns in the test set caused by dummification\n",
    "missing_cols = set(train_predictors) - set(test_predictors)\n",
    "# Add missing columns to test set with default value equal to 0\n",
    "for c in missing_cols:\n",
    "    test_predictors[c] = 0\n",
    "# Ensure the order of column in the test set is in the same order than in train set\n",
    "test_predictors = test_predictors[train_predictors.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(train_predictors, y_train)\n",
    "pred_train = dt.predict(train_predictors)\n",
    "pred_test = dt.predict(test_predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_train, pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation\n",
    "\n",
    "A basic decision tree is overfitting on the train data. Let's see if a Random Forest ensemble model can do better.\n",
    "\n",
    "Below are the results of Random Forest after manual hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(criterion = \"gini\", n_estimators = 250, max_depth = 10, \n",
    "                            max_features = 50, min_samples_split = 4, random_state = 0)\n",
    "rf.fit(train_predictors, y_train)\n",
    "pred_train = rf.predict(train_predictors)\n",
    "pred_test = rf.predict(test_predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_train, pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohen_kappa_score(y_train, pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohen_kappa_score(y_test, pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_train, pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting class counts in the train predictions\n",
    "np.unique(pred_train, return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing the same for the test predictions\n",
    "np.unique(pred_test, return_counts = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RF classifier has not predicted any samples for the minority class (0) in the test data, which means it has not learnt that class. \n",
    "\n",
    "Let's rebuild the Random Forest with a class weight parameter to handle class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(criterion = \"gini\", n_estimators = 250, max_depth = 10, \n",
    "                            max_features = 50, min_samples_split = 4, random_state = 0,\n",
    "                           class_weight = \"balanced\")\n",
    "rf.fit(train_predictors, y_train)\n",
    "pred_train = rf.predict(train_predictors)\n",
    "pred_test = rf.predict(test_predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting class counts in the train predictions\n",
    "np.unique(pred_train, return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing the same for the test predictions\n",
    "np.unique(pred_test, return_counts = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new RF is assigning a **huge** number of samples to the minority class!\n",
    "\n",
    "Here are the actual class counts for train and test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_train, return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_test, return_counts = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging may not be enough for this classification task. Let's try boosting with XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building an XGBoost classifier\n",
    "xgb = XGBClassifier(n_estimators = 250, max_depth = 20, gamma = 2, learning_rate = 0.001, random_state = 0)\n",
    "\n",
    "xgb.fit(train_predictors, y_train)\n",
    "pred_train = xgb.predict(train_predictors)\n",
    "pred_test = xgb.predict(test_predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_train, pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohen_kappa_score(y_train, pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohen_kappa_score(y_test, pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_train, pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting class counts in the train predictions\n",
    "np.unique(pred_train, return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing the same for the test predictions\n",
    "np.unique(pred_test, return_counts = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see which features our classifier found most important for rating class prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing a feature importances plot\n",
    "\n",
    "plt.figure(figsize = (20, 10))\n",
    "feat_importances = pd.Series(xgb.feature_importances_, index=train_predictors.columns)\n",
    "feat_importances.nlargest(20).plot(kind='barh')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results summary\n",
    "\n",
    "In this kernel we split the restaurant ratings into four ranges (classes) and built tree-based classifiers to predict them. **The best performer was a manually tuned XGBoost classifier with 72% accuracy on train and 67% accuracy on test.** But since the rating classes are somewhat imbalanced, we also evaluated our predictions with Cohen Kappa and F1 scores.\n",
    "\n",
    "The Cohen Kappa score was 0.56 on train and 0.47 on test. This can be interpreted as the model **performing moderately well** (according to Landis & Koch). Average weighted F1 score was 0.69 and 0.64 on train and test respectively. \n",
    "\n",
    "Random Forest also gave similar scores but the class counts in RF predictions showed that minority classes were being misclassified, i.e. almost all restaurants with \"rare\" ratings were being misclassified as having a more \"common\" rating. To counter this, we added the parameter (class_weight = \"balanced\") to the RF. This made the classifier overcompensate for the minority classes (incorrectly classified too many restaurants into the minority classes) and brought down the scores. \n",
    "\n",
    "XGBoost with manually tuned depth, learning rate and gamma (regularization) hyperparameters predicted more minority samples correctly than RF. **The most important feature for prediction was Votes, followed by rest_type_Dessert_Parlor. **\n",
    "\n",
    "In Part Three of my analysis we will apply text mining techniques to extract insights from textual features, like customer reviews, and attempt rating classification with a neural network."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
