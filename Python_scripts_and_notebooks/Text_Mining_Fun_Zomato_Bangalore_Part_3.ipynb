{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fish fry, chicken fried rice, masala dosa, paneer tikka. Can you guess which of these food items made it to the top 50 mentions in Zomato Bangalore customer reviews?\n",
    "\n",
    "This is Part Three of my analysis of the Zomato Bangalore dataset. In Part One we explored the data and predicted restaurant ratings with six selected features using Linear Regression, while in Part Two we predicted the ratings (split into classes) using classification models. \n",
    "\n",
    "In this kernel we will apply text mining / NLP techniques to extract insights from textual features like customer reviews. Then we will try to predict restaurant ratings by feeding the transformed text to a neural network.\n",
    "\n",
    "This kernel consists of:\n",
    "\n",
    "- Data cleaning (identifying and dropping duplicates, selecting features)\n",
    "- Text mining and insights (ngrams, bigrams, trigrams and FreqDist plots)\n",
    "- Text processing (regex, tokenizing, stopword removal, lemmatizing, vectorizing)\n",
    "- Building an LSTM Neural Network \n",
    "- Model evaluation\n",
    "- Results summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.corpus import RegexpTokenizer as regextoken\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist, bigrams, trigrams\n",
    "from nltk import WordNetLemmatizer\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import gensim\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Flatten, Embedding, Conv1D, MaxPooling1D, Dropout, LSTM, GRU\n",
    "from keras.regularizers import l1, l2\n",
    "from sklearn.metrics import classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "zomato = pd.read_csv(\"../input/zomato-bangalore-restaurants/zomato.csv\", na_values = [\"-\", \"\"])\n",
    "# Making a copy of the data to work on\n",
    "data = zomato.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping duplicates - see Part One for explanation\n",
    "\n",
    "grouped = data.groupby([\"name\", \"address\"]).agg({\"listed_in(type)\" : list})\n",
    "newdata = pd.merge(grouped, data, on = ([\"name\", \"address\"]))\n",
    "newdata[\"listed_in(type)_x\"] = newdata[\"listed_in(type)_x\"].astype(str) # converting unhashable list to a hashable type\n",
    "newdata.drop_duplicates(subset = [\"name\", \"address\", \"listed_in(type)_x\"], inplace = True)\n",
    "newdata = newdata.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming the ratings column \n",
    "\n",
    "newdata[\"rating\"] = newdata[\"rate\"].str[:3] # Extracting the first three characters of each string in \"rate\"\n",
    "# Removing rows with \"NEW\" in ratings as it is not a predictable level\n",
    "newdata = newdata[newdata.rating != \"NEW\"] \n",
    "# Dropping rows that have missing values in ratings \n",
    "newdata = newdata.dropna(subset = [\"rating\"])\n",
    "# Converting ratings to a numeric column so we can discretize it\n",
    "newdata[\"rating\"] = pd.to_numeric(newdata[\"rating\"])\n",
    "# Discretizing the ratings into a categorical feature with 4 levels\n",
    "\n",
    "newdata[\"rating\"] = pd.cut(newdata[\"rating\"], bins = [0, 3.0, 3.5, 4.0, 5.0], labels = [\"0\", \"1\", \"2\", \"3\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our four rating bins (classes) will be 0 to 3 < 3 to 3.5 < 3.5 to 4 < 4 to 5. To make label encoding easier later, we'll label these classes 0, 1, 2, 3. We can think of these as Very Low, Low, Medium and High."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the rating class distribution\n",
    "plt.figure(figsize = (10, 5))\n",
    "sns.countplot(newdata[\"rating\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "newdata.describe(include = \"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agenda\n",
    "We will use the reviews_list, menu_item, dish_liked and cuisines columns for our analysis.\n",
    "\n",
    "First, we will look at the customer reviews and pull out the most common words and phrases. Next, we will analyse cuisine listings and identify cuisines that are rare in Bangalore. Finally we will build a neural network with all four features to predict restaurant ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new dataset that has only customer reviews and restaurant ratings\n",
    "reviews_data = newdata[[\"reviews_list\", \"rating\"]]\n",
    "# Examining the reviews for the first restaurant in the dataset\n",
    "reviews_data[\"reviews_list\"][0]\n",
    "# The text needs cleaning up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting all the text to lowercase\n",
    "reviews_data[\"reviews_list\"] = reviews_data[\"reviews_list\"].apply(lambda x: x.lower())\n",
    "\n",
    "# Creating a regular expression tokenizer that matches only alphabets\n",
    "# This will return separate words (tokens) from the text\n",
    "tokenizer = regextoken(\"[a-zA-Z]+\") \n",
    "# Applying the tokenizer to each row of the reviews\n",
    "review_tokens = reviews_data[\"reviews_list\"].apply(tokenizer.tokenize)\n",
    "# Examining the tokens created for the first row / restaurant\n",
    "print(review_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing and examining the English stopwords directory \n",
    "# These are common words that typically don't add meaning to the text and can be removed\n",
    "stop = stopwords.words(\"english\")\n",
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding custom words to stopwords \n",
    "stop.extend([\"rated\", \"n\", \"nan\", \"x\"])\n",
    "# Removing stopwords from the tokens\n",
    "review_tokens = review_tokens.apply(lambda x: [token for token in x if token not in stop])\n",
    "# Concatenating all the reviews \n",
    "all_reviews = review_tokens.astype(str).str.cat()\n",
    "cleaned_reviews = tokenizer.tokenize(all_reviews)\n",
    "\n",
    "# Getting the frequency distribution of individual words in the reviews\n",
    "fd = FreqDist()\n",
    "for word in cleaned_reviews:\n",
    "    fd[word] += 1\n",
    "    \n",
    "# Examining the top 5 most frequent words\n",
    "fd.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the top 50 most frequent words\n",
    "plt.figure(figsize = (10, 5))\n",
    "fd.plot(50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "Of the 50 most frequent words across customer reviews, six reveal food preferences: **chicken, biryani, veg, pizza, rice, paneer**. The only negative word in the top 50 is \"bad\".\n",
    "\n",
    "Factors contributing to restaurant experience are mentioned in the following (descending) order of frequency: place > taste > service > time > ambience > staff > quality > delivery > menu > quantity > friendly.\n",
    "\n",
    "Now let us repeat the analysis on a bi-gram level. Bi-grams are pairs of words which can provide better context than individual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating bigrams from the reviews\n",
    "bigrams = bigrams(cleaned_reviews)\n",
    "# Getting the bigram frequency distribution\n",
    "fd_bigrams = FreqDist()\n",
    "for bigram in bigrams:\n",
    "    fd_bigrams[bigram] += 1\n",
    "# Examining the top 5 most frequent bigrams\n",
    "fd_bigrams.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the top 50 most frequent bigrams\n",
    "plt.figure(figsize = (10, 5))\n",
    "fd_bigrams.plot(50)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "We have some new insights! Food items/preferences mentioned in the top 50 bigrams are **ice cream, non veg, North Indian, chicken biryani, fried rice, chicken and South Indian**. Top six bigrams related to restaurant experience: good food > good place > good service > value (for) money > pocket friendly > ambience good. \n",
    "\n",
    "There's a key insight here: **the expense factor, which was missed by individual word frequency counts, was picked up by the bigram frequency counts.**\n",
    "\n",
    "Zomato might also be happy to know their membership program \"Zomato Gold\" is in the top 50 bigrams, with 2593 mentions in the customer reviews.\n",
    "\n",
    "What about trigrams? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating trigrams from the reviews\n",
    "trigrams = trigrams(cleaned_reviews)\n",
    "\n",
    "fd_trigrams = FreqDist()\n",
    "for trigram in trigrams:\n",
    "    fd_trigrams[trigram] += 1\n",
    "\n",
    "fd_trigrams.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 5))\n",
    "fd_trigrams.plot(50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "There appears to be some bad data (strings of \"xa xa xa\") somewhere in the reviews, but we'll ignore that. The specific food preferences we can see here are **paneer butter masala, chicken fried rice, chicken biryani, peri peri chicken and chicken ghee roast**. Bangalore is really into chicken.\n",
    "\n",
    "On restaurant experience: a specific insight revealed by the trigrams is that **many people are looking for places to hang out with their friends**. \n",
    "\n",
    "We also see a variety of positive trigrams like \"must visit place\", \"food really good\", \"service also good\" and \"worth every penny\". However, there is only one negative trigram in the top 50 - \"worst food ever\".\n",
    "\n",
    "We now have plenty of insights into customer preferences and experiences, and will move onto an analysis of Bangalore's cuisines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new dataset with cuisines info and restaurant ratings\n",
    "cuisines = newdata[[\"cuisines\", \"rating\"]]\n",
    "cuisines[\"cuisines\"] = cuisines[\"cuisines\"].astype(str)\n",
    "# Converting to lowercase\n",
    "cuisines[\"cuisines\"] = cuisines[\"cuisines\"].apply(lambda x: x.lower())\n",
    "# Tokenizing the cuisines\n",
    "cuisine_tokens = cuisines[\"cuisines\"].apply(tokenizer.tokenize)\n",
    "# Concatenating all the cuisine names into one text document\n",
    "all_cuisines = cuisine_tokens.astype(str).str.cat()\n",
    "cleaned_cuisines = tokenizer.tokenize(all_cuisines)\n",
    "\n",
    "# Generating cuisine frequencies \n",
    "fd_cuisine = FreqDist()\n",
    "for cuisine in cleaned_cuisines:\n",
    "    fd_cuisine[cuisine] += 1\n",
    "    \n",
    "# Printing the 50 most common cuisines (top 50)\n",
    "print(fd_cuisine.most_common(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "One must be careful when interpreting these lists. For example, \"dogs\" can't be a cuisine but the preceding word \"hot\" tells us that the cuisine is \"hot dogs\". Another tricky one is Cantonese, which comes under Chinese and so might not really be rare.\n",
    "\n",
    "We've done our reviews and cuisines analysis and will now prepare all the text in the dataset for feeding into a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting all the text to strings\n",
    "newdata[[\"reviews_list\", \"menu_item\", \"dish_liked\", \"cuisines\"]] = newdata[[\"reviews_list\", \"menu_item\", \"dish_liked\", \"cuisines\"]].astype(\"str\")\n",
    "# Combining all the text data into a single feature called \"text\"\n",
    "newdata[\"text\"] = newdata[\"reviews_list\"] + \" \" + newdata[\"menu_item\"] + \" \" + newdata[\"dish_liked\"] + \" \" + newdata[\"cuisines\"]\n",
    "# Creating a new dataset with text and restaurant ratings\n",
    "text_data = newdata[[\"text\", \"rating\"]]\n",
    "# Converting text to lowercase\n",
    "text_data[\"text\"] = text_data[\"text\"].apply(lambda x: x.lower())\n",
    "# Tokenizing the text\n",
    "tokens = text_data[\"text\"].apply(tokenizer.tokenize) \n",
    "# Removing stopwords \n",
    "tokens = tokens.apply(lambda x: [token for token in x if token not in stop])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing a function to lemmatize words\n",
    "lmtzr = WordNetLemmatizer()\n",
    "def lem(text):\n",
    "    return [lmtzr.lemmatize(word) for word in text]\n",
    "\n",
    "# Applying the function to each row of the text\n",
    "# i.e. reducing each word to its lemma\n",
    "tokens_new = tokens.apply(lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying label encoding and one hot encoding to the restaurant rating classes \n",
    "le = LabelEncoder()\n",
    "target = le.fit_transform(text_data[\"rating\"])\n",
    "target = to_categorical(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into train and test sets (stratified)\n",
    "X_train, X_test, y_train, y_test = train_test_split(tokens_new, target, test_size = 0.3, random_state = 0, stratify = target)\n",
    "\n",
    "# Processing the text with the Keras tokenizer\n",
    "t = Tokenizer() \n",
    "t.fit_on_texts(X_train)\n",
    "# Setting a vocabulary size that we will specify in the neural network\n",
    "vocab_size = len(t.word_index) + 1\n",
    "# The t.word_index contains each unique word in our text and an integer assigned to it\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding the text as sequences of integers\n",
    "train_sequences = t.texts_to_sequences(X_train)\n",
    "test_sequences = t.texts_to_sequences(X_test)\n",
    "# Adding zeros so each sequence has the same length \n",
    "train_padded = pad_sequences(train_sequences, maxlen=500)\n",
    "test_padded = pad_sequences(test_sequences, maxlen=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embedding \n",
    "We'll use Google's pre-trained Word2Vec word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Word2Vec word embeddings \n",
    "\n",
    "word_vectors = KeyedVectors.load_word2vec_format('../input/googlenewsvectorsnegative300/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "embedding_dim = 300 # each word will become a 300-d vector\n",
    "\n",
    "# Creating an empty matrix \n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim)) \n",
    "# Each row is a word with 300 dimensions\n",
    "\n",
    "# Populating the matrix\n",
    "for word, i in t.word_index.items(): # for each word in the customer reviews vocabulary\n",
    "    try:\n",
    "        # get the Word2Vec vector representation for that word\n",
    "        embedding_vector = word_vectors[word] \n",
    "        # add it to the embedding matrix\n",
    "        embedding_matrix[i] = embedding_vector \n",
    "        # handle new words by generating random vectors for them\n",
    "    except KeyError: \n",
    "        embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25), embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examining the words embeddings - vector representations of words\n",
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building an LSTM neural network\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "max_length = 500 # maximum length of each input string (movie review)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim, input_length=max_length, weights = [embedding_matrix], trainable = False))\n",
    "model.add(LSTM(100, activation = \"tanh\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer = \"adam\", metrics=['accuracy'])\n",
    "model.fit(train_padded, y_train, validation_data=(test_padded, y_test), epochs=15, batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting on the train data\n",
    "pred_train = model.predict(train_padded)\n",
    "pred_train = np.argmax(pred_train, axis=1)\n",
    "y_train = np.argmax(y_train, axis=1)\n",
    "# Printing evaluation metrics\n",
    "print(classification_report(y_train, pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting on the test data\n",
    "pred_test = model.predict(test_padded)\n",
    "pred_test = np.argmax(pred_test, axis=1)\n",
    "y_test = np.argmax(y_test, axis = 1)\n",
    "# Printing evaluation metrics\n",
    "print(classification_report(y_test, pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results summary\n",
    "By applying text mining techniques to customer reviews and other data, **we discovered common food preferences which became more specific as we progressed (dish names)**. **We also noted which aspects of a restaurant people care about and in what order of priority. Then we identified the most and least common cuisines in the city along with their prevalence.**\n",
    "\n",
    "The text mining activity was interesting from both a data science perspective and a business perspective, as it showed the usefulness of different NLTK tools and revealed actionable insights.\n",
    "\n",
    "After processing the text, we fed it to an LSTM network with pre-trained Word2Vec word vectors to see if we could predict the four rating classes we created. Accuracy and average F1 scores were lower than what we got with XGBoost in Part Two of my analysis. \n",
    "\n",
    "The conclusion from this three-part analysis is that non-text features and tree-based classification models do a better job of predicting the restaurant ratings than LSTM with text features.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
